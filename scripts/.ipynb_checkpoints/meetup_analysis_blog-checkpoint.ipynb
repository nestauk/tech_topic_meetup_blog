{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing of Meetup data for Nesta blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import ratelim\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#File paths\n",
    "#Path to data files\n",
    "\n",
    "main_directory = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.dirname(os.getcwd()) + \"/\" + \"meetup_data\"\n",
    "intermediate_output_path = os.path.dirname(os.getcwd()) + \"/\" + \"intermediate_output\"\n",
    "\n",
    "#Read api key from config file 'my_api_key'\n",
    "with open(os.path.join(main_directory,\"my_api_key.json\"),'r') as data_file:\n",
    "    my_api_key = json.load(data_file)['api_key']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Functions for data crawling\n",
    "\n",
    "#Meetup API url\n",
    "api_base_url = \"https://api.meetup.com/2/\"\n",
    "\n",
    "#Rate limits\n",
    "RATELIM_DUR = 60 * 60\n",
    "RATELIM_QUERIES = 9000\n",
    "\n",
    "#Event crawler\n",
    "@ratelim.patient(RATELIM_QUERIES,RATELIM_DUR)\n",
    "def crawl_events(group_id):\n",
    "    '''\n",
    "    Input: a meetup group id\n",
    "    Output: a json object with information about the group events.\n",
    "    '''\n",
    "    \n",
    "    #Build request\n",
    "    api_url = api_base_url + 'events'\n",
    "    request_parameters = \"?group_id={}&status=past&key={}\".format(group_id,my_api_key)\n",
    "    \n",
    "    event_request = api_url + request_parameters\n",
    "    \n",
    "    #Make API call and obtain response using the get method in requests.\n",
    "    response = requests.get(event_request)\n",
    "    \n",
    "    return(response.json())\n",
    "\n",
    "#RSVP crawler. NB we haven't used this function in this analysis. \n",
    "@ratelim.patient(RATELIM_QUERIES,RATELIM_DUR)\n",
    "def crawl_rspvs(event_id):\n",
    "    '''\n",
    "    Input: event_id\n",
    "    Output: a json object with information about attendees at events\n",
    "    '''\n",
    "    \n",
    "    #Build request:\n",
    "    api_url = api_base_url + \"rsvps\"\n",
    "    request_parameters = \"?&event_id={}&key={}\".format(event_id,my_api_key)\n",
    "    \n",
    "    rsvps_request = api_url + request_parameters\n",
    "    \n",
    "    #Make request\n",
    "    response = requests.get(rsvps_request)\n",
    "    \n",
    "    return(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utilities to process data\n",
    "def extract_topics_from_dict(topic,container):\n",
    "    '''\n",
    "    This function goes through a list where each element is a topic with a dictionary containing name,\n",
    "        urlkey (unique identifier) and id. It extracts the urlkey.\n",
    "    \n",
    "    input: a container (list) of dictionaries and a key (topic) to extract\n",
    "    output: a list with the topics\n",
    "    '''\n",
    "    \n",
    "    out = [top[topic] for top in container]\n",
    "    return(out)\n",
    "\n",
    "#Work with datetimes\n",
    "def extract_date_from_epoch(posix_date):\n",
    "    '''\n",
    "    input: a POSIX timestamp.\n",
    "    output: a local date\n",
    "    '''\n",
    "    out = datetime.datetime.fromtimestamp(posix_date).strftime(\"%d-%m-%Y\")\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def get_month(date_var):\n",
    "    '''\n",
    "    input: a date\n",
    "    returnd: A date that bins all days for that month into a single month (day 1)\n",
    "    '''\n",
    "    outvar = datetime.datetime.strptime(\"01-\" + date_var[3:],\"%d-%m-%Y\")  \n",
    "    return(outvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions to extract groups from \n",
    "def extract_groups_with_keyword(kw):\n",
    "    \"\"\"\n",
    "    input: a keyword kw\n",
    "    returns: A df with a boolean columns indicating if kw is in the keyword list for the group\n",
    "    \"\"\"\n",
    "    group_topics_df['has_keyword'] = [kw in \n",
    "                              topic_set for topic_set in group_topics_df['group_topics']]\n",
    "    \n",
    "    return(group_topics_df)     \n",
    "    \n",
    "def get_groups_with_keyword(kw):\n",
    "    '''\n",
    "    Returns the group metadata for groups that contain a keyword.\n",
    "    \n",
    "    Input: kw, the keyword\n",
    "    Output: A dataframe with group metadata.\n",
    "    \n",
    "    '''\n",
    "    #Extract dataframe with labelled groups\n",
    "    domain_df = extract_groups_with_keyword(kw)\n",
    "    \n",
    "    #Subset by variable of interest\n",
    "    domain_mask = domain_df['has_keyword']==True\n",
    "    \n",
    "    #Extract relevant ids\n",
    "    domain_ids = list(domain_df.ix[domain_mask,'group_id'])\n",
    "    \n",
    "    #Extract information about relevant ids\n",
    "    domain_groups_df = group_metadata_df[[gid in domain_ids \n",
    "                                       for gid in group_metadata_df['group_id']]]\n",
    "    \n",
    "    domain_groups_df['topic_id'] = kw\n",
    "    \n",
    "    #Return data\n",
    "    return(domain_groups_df)\n",
    "\n",
    "#Function to extract all event activity for a keyword\n",
    "def extract_keyword_activity(keyword):\n",
    "    '''\n",
    "    Input: a meetup keyword\n",
    "    Output: Outputs with event activity: a df with groups active in the keyword, and events for those groups.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Extract groups\n",
    "    groups_df = get_groups_with_keyword(keyword)\n",
    "    \n",
    "    #Extract group ids\n",
    "    group_ids = list(groups_df['group_id'])\n",
    "    \n",
    "    #Extract events and their results\n",
    "    events = [crawl_events(i) for i in group_ids]\n",
    "    \n",
    "    #Extract event ids and their rsvps\n",
    "    event_results = [event['results'] for event in events if\n",
    "                       len(event['results'])>0]\n",
    "    event_ids= [event['id'] for event_list in event_results for event in event_list]\n",
    "\n",
    "    #Crawl event ids\n",
    "    #rsvps = [crawl_rspvs(i) for i in event_ids]\n",
    "    \n",
    "    #Parse events\n",
    "    event_results = [{\n",
    "        \"event_id\":event_result['id'],\n",
    "        \"date\":extract_date_from_epoch(int(event_result['time'])/1000),\n",
    "        \"group_id\": event_result['group']['id'],\n",
    "        \"attendees\":event_result['yes_rsvp_count']} for\n",
    "                     event_list in event_results for event_result in event_list]\n",
    "    event_df = pd.DataFrame(event_results)\n",
    "    \n",
    "    if (len(event_df)>0):\n",
    "        events_labelled_df = pd.merge(event_df,\n",
    "                                    groups_df[['group_id','topic_id']],\n",
    "                                     left_on='group_id',\n",
    "                                     right_on='group_id')\n",
    "\n",
    "        outputs = [events_labelled_df]\n",
    "        return(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Data processing: Generate dataset with group/event activity in selected keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The input for this analysis is a json file with information about tech groups obtained with\n",
    "#a Meetup API wrapper developed by Matt Williams: https://github.com/mattjw/exploring_tech_meetups\n",
    "\n",
    "#Observations: json loads works with strings so we had to decode the lines.\n",
    "tech_groups = [json.loads(line.decode()) for line in open(data_path + \"/\" + \"tech_groups.json\",\"rb\")]\n",
    "\n",
    "#Group topics is a lis of dict with group ids, date when they were created, and their topics.\n",
    "group_topics = [{\"group_id\": g[\"_id\"],\n",
    "                 \"group_created\": extract_date_from_epoch(\n",
    "            int(g[\"created\"]['$numberLong'])/1000),\n",
    "                   \"group_topics\":extract_topics_from_dict('urlkey',g['topics'])} for\n",
    "                g in tech_groups]\n",
    "#Create dataframe\n",
    "group_topics_df = pd.DataFrame(group_topics)\n",
    "\n",
    "#Create a month bin where day is always one\n",
    "group_topics_df['created_date'] = group_topics_df[\n",
    "    'group_created'].apply(lambda x:\n",
    "                           datetime.datetime.strptime(\"01-\" + x[3:],\"%d-%m-%Y\"))  \n",
    "\n",
    "#Create list of documents with keywords\n",
    "group_topics_list = [g['group_topics'] for g in group_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse group metadata\n",
    "group_metadata_df = pd.DataFrame([{\"group_id\": g[\"_id\"],\n",
    "                        \"group_name\": g[\"name\"],\n",
    "                        \"group_city\": g[\"city\"],\n",
    "                        \"group_lon\":g[\"lon\"],\n",
    "                        \"group_lat\":g[\"lat\"],\n",
    "                        \"group_created\": \n",
    "                                   extract_date_from_epoch(int(g[\"created\"]['$numberLong'])/1000),\n",
    "                   \"group_topics\":extract_topics_from_dict('urlkey',g['topics'])} for\n",
    "                                   g in tech_groups])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Generate dataframe with groups in the keywords of interest (they could be something else)\n",
    "domain_list = ['virtual-reality','deep-learning','bitcoin']\n",
    "\n",
    "domain_dfs = [get_groups_with_keyword(x) for x in\n",
    "                                      domain_list]\n",
    "\n",
    "all_domains_df = pd.concat(domain_dfs,axis=0)\n",
    "\n",
    "#Output\n",
    "all_domains_df.to_csv(os.path.join(intermediate_output_path,\"domain_activity_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract events and RSVPs for all relevant groups\n",
    "\n",
    "#Extract ids for all relevant groups\n",
    "domain_gr_ids = list(all_domains_df['group_id'])\n",
    "\n",
    "#Crawl events\n",
    "domain_events = [crawl_events(i) for i in domain_gr_ids]\n",
    "\n",
    "#Extract results (for events where there are results)\n",
    "domain_event_results = [event['results'] for event in domain_events if\n",
    "                       len(event['results'])>0]\n",
    "domain_event_ids= [event['id'] for event_list in domain_event_results for event in event_list]\n",
    "\n",
    "#Each group contains a list of events. We extract their date, rsvp count and location.\n",
    "all_event_results = [{\n",
    "        \"event_id\":event_result['id'],\n",
    "        \"date\":extract_date_from_epoch(int(event_result['time'])/1000),\n",
    "        \"group_id\": event_result['group']['id'],\n",
    "        \"attendees\":event_result['yes_rsvp_count']} for\n",
    "                     event_list in domain_event_results for event_result in event_list]\n",
    "\n",
    "#Put everythingin a dataframe\n",
    "all_event_df = pd.DataFrame(all_event_results)\n",
    "\n",
    "all_events_labelled_df = pd.merge(all_event_df,\n",
    "                                all_domains_df[['group_id','topic_id']],\n",
    "                                 left_on='group_id',\n",
    "                                 right_on='group_id')\n",
    "all_events_labelled_df.to_csv(os.path.join(intermediate_output_path,\"domain_events_df.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Data processing: Generate processed outputs for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=4).mean()\n"
     ]
    }
   ],
   "source": [
    "#Generate df with counts of groups formed and attendees at events by keyword/month combination\n",
    "#Create month variable\n",
    "all_events_labelled_df['month_date'] = all_events_labelled_df['date'].map(lambda x:\n",
    "                                                                         get_month(x))\n",
    "#Group_by month and generate summaries\n",
    "\n",
    "#Create dict with functions per variable. We want, for each month/keyword combo,\n",
    "#    a count of all groups created and the sum of all attendees to events in those groups\n",
    "summ_funcs = {'group_id': ['count'],'attendees': ['sum']}\n",
    "\n",
    "#Group by month and extract summary statistics.\n",
    "all_activity_df = all_events_labelled_df.groupby([\n",
    "        'month_date','topic_id']).agg(summ_funcs).reset_index(drop=False)\n",
    "\n",
    "#We have multi-index columns. Drop the lower index\n",
    "all_activity_df.columns = all_activity_df.columns.droplevel(level=1)\n",
    "\n",
    "#Pivot table\n",
    "all_activity_pivot = all_activity_df.pivot_table(columns='topic_id',\n",
    "                                                 index='month_date',\n",
    "                                                 values=['attendees','group_id'])\n",
    "all_activity_pivot.fillna(value=0,inplace=True)\n",
    "all_act_rm = all_activity_pivot.apply(lambda x: pd.rolling_mean(x,window=4))\n",
    "\n",
    "#Create additional metrics for visualisation, and output.\n",
    "all_event_metrics = pd.melt(all_act_rm.reset_index(drop=False),\n",
    "                            id_vars='month_date')\n",
    "all_event_metrics.rename(columns={None:\"metric\"},inplace=True)\n",
    "\n",
    "all_event_pivot = pd.pivot_table(all_event_metrics,\n",
    "                                index=['month_date','topic_id'],columns='metric',\n",
    "                                values='value')\n",
    "\n",
    "#Average attendees\n",
    "all_event_pivot['average_attendees'] = all_event_pivot['attendees']/all_event_pivot['group_id']\n",
    "all_event_pivot.fillna(value=0,inplace=True)\n",
    "all_event_pivot.reset_index(level=1,drop=False,inplace=True)\n",
    "\n",
    "\n",
    "#my_mask = [x.year >= 2013 for x in all_event_pivot.index]\n",
    "#all_event_pivot = all_event_pivot[my_mask]\n",
    "\n",
    "all_event_pivot.reset_index(drop=False,inplace=True)\n",
    "all_event_pivot.to_csv(os.path.join(intermediate_output_path,\"domain_activity_not_norm.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:35: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with \n",
      "\tSeries.rolling(center=False,window=4).mean()\n"
     ]
    }
   ],
   "source": [
    "#We want to normalise all the above by 'average' levels of activity in Meetup, based on 200 randomly selected\n",
    "#groups.\n",
    "random.seed(123)\n",
    "\n",
    "#Extract random sample of 200 groups\n",
    "#Extract 200 random indices based on the tech_groups list\n",
    "random_groups = random.sample(range(0,len(tech_groups)),200)\n",
    "\n",
    "#Get the ids for those\n",
    "random_group_ids = [g['id'] for num,g in enumerate(tech_groups) if num in random_groups]\n",
    "\n",
    "#Extract them from Meetup\n",
    "random_event_results = [crawl_events(gid)['results'] for gid in random_group_ids]\n",
    "\n",
    "#Create df\n",
    "random_events_df = pd.DataFrame([{\"event_id\":event_result['id'],\n",
    "                    \"date\":extract_date_from_epoch(int(event_result['time'])/1000),\n",
    "                    \"group_id\": event_result['group']['id'],\n",
    "                    \"attendees\":event_result['yes_rsvp_count']} for\n",
    "                     event_list in random_event_results for event_result in event_list])\n",
    "\n",
    "#Same processing as we did above, yoink repetitive.\n",
    "#Extract month\n",
    "random_events_df['month_date'] = random_events_df['date'].map(lambda x:\n",
    "                                                                         get_month(x))\n",
    "\n",
    "#Extract summary stats (not aggregating by keyword because it is unnecessary)\n",
    "random_activity_df = random_events_df.groupby([\n",
    "        'month_date']).agg(summ_funcs)\n",
    "\n",
    "random_activity_df.columns = random_activity_df.columns.droplevel(level=1)\n",
    "\n",
    "random_activity_df = random_activity_df[['attendees','group_id']]\n",
    "\n",
    "random_rolling = random_activity_df.apply(lambda x: pd.rolling_mean(x,window=4))\n",
    "\n",
    "#Create domain activity dataframe for plotting (normalised)\n",
    "\n",
    "random_rolling['average_attendees']=random_rolling['attendees']/random_rolling['group_id']\n",
    "random_rolling['topic_id'] = 'all'\n",
    "random_rolling.reset_index(drop=False,inplace=True)\n",
    "\n",
    "#Process: melt both, concatenate over rows and pivot\n",
    "random_melted = pd.melt(random_rolling,\n",
    "                        id_vars=['month_date','topic_id'])\n",
    "random_melted.rename(columns={'variable':'metric'},inplace=True)\n",
    "\n",
    "\n",
    "all_event_melted = pd.melt(all_event_pivot,\n",
    "                           id_vars=['month_date','topic_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalise the domain df by activity in the random set\n",
    "\n",
    "event_random_long = pd.merge(all_event_melted,random_melted,left_on=['month_date','metric'],\n",
    "                            right_on=['month_date','metric'],how='inner')\n",
    "\n",
    "event_random_norm = event_random_wide.apply(\n",
    "    lambda x: x/event_random_wide['all']).reset_index(level=1,drop=False)\n",
    "\n",
    "#my_mask = [np.isnan()] >= 2013 for x in event_random_norm.index]\n",
    "\n",
    "all_event_norm = event_random_norm.ix[:,['metric','bitcoin','deep-learning',\n",
    "                                              'virtual-reality']].reset_index(drop=False)\n",
    "all_event_norm.to_csv(os.path.join(intermediate_output_path,\"domain_activity_norm.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: Extract data about emerging trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the top 10 keywords for 2015 and look at their number of events and number of attendees.\n",
    "\n",
    "def extract_novel_keywords(threshold_date):\n",
    "    '''\n",
    "    input: a threshold date (string) in %d-%m-%Y format\n",
    "    output: novel keywords after the threshold.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    threshold_date = datetime.datetime.strptime(threshold_date,\"%d-%m-%Y\")\n",
    "\n",
    "    #Unique kws before threshold\n",
    "    first_period_kws = set([kw for kw_list,date in \n",
    "                           zip(group_topics_df['group_topics'],\n",
    "                               group_topics_df['created_date']) for kw in kw_list if\n",
    "                            date < threshold_date])\n",
    "\n",
    "    snd_period_kws = set([kw for kw_list,date in \n",
    "                           zip(group_topics_df['group_topics'],\n",
    "                               group_topics_df['created_date']) for kw in kw_list if\n",
    "                           date > threshold_date])\n",
    "\n",
    "    novel_kws = [tag for tag in snd_period_kws if tag not in first_period_kws]\n",
    "\n",
    "    #Count novel kws\n",
    "    novel_counts = [kw for kw_list,date in \n",
    "                           zip(group_topics_df['group_topics'],\n",
    "                               group_topics_df['created_date']) for kw in kw_list if\n",
    "                           date > threshold_date and kw in novel_kws]\n",
    "\n",
    "    #Count all kw appearances in period\n",
    "    novel_freqs = pd.Series(novel_counts).value_counts()\n",
    "\n",
    "    return(novel_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract top 10 keywords from last year.\n",
    "top_10_last_year = extract_novel_keywords(\"01-03-2015\")[:20]\n",
    "\n",
    "top_10_keywords_list = [i for i in top_10_last_year.index]\n",
    "\n",
    "recent_keywords_info = [extract_keyword_activity(i) for i in top_10_keywords_list]\n",
    "recent_activity_df = pd.concat([x[0] for x in recent_keywords_info],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recent_activity_df for plotting with R (similar processing to above, and some repetition)\n",
    "\n",
    "summ_funcs = {'group_id': ['count','unique'],'attendees': ['sum']}\n",
    "\n",
    "recent_activity_df['month_year'] = recent_activity_df['date'].apply(get_month)\n",
    "\n",
    "recent_activity_stats= recent_activity_df.groupby(['topic_id']).agg(summ_funcs)\n",
    "\n",
    "recent_activity_stats.columns = recent_activity_stats.columns.droplevel(level=0)\n",
    "\n",
    "recent_activity_stats['group_number'] = [len(x) for x in recent_activity_stats['unique']]\n",
    "\n",
    "recent_activity_stats.rename(columns={'count':'event_number','sum':'attendees'},inplace=True)\n",
    "\n",
    "recent_activity_stats = recent_activity_stats[['event_number','group_number','attendees']]\n",
    "\n",
    "#New variablws\n",
    "recent_activity_stats['attendees_per_event'] = recent_activity_stats['attendees']/recent_activity_stats['event_number']\n",
    "recent_activity_stats['events_per_group'] = recent_activity_stats['event_number']/recent_activity_stats['group_number']\n",
    "\n",
    "#Write out\n",
    "recent_activity_stats.to_csv(os.path.join(intermediate_output_path,\"recent_activity.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
